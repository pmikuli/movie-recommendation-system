{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-09T15:59:41.497513Z",
     "start_time": "2025-05-09T15:59:41.492776Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T15:59:41.703491Z",
     "start_time": "2025-05-09T15:59:41.697817Z"
    }
   },
   "id": "685cb143705718b4",
   "execution_count": 209
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_users = pd.read_parquet('user_features_clean.parquet')\n",
    "df_movies = pd.read_parquet('Movies_clean_Vec_v4_25keywords.parquet')\n",
    "df_ratings = pd.read_parquet('ratings_groupped_ids.parquet')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T15:59:44.232492Z",
     "start_time": "2025-05-09T15:59:41.733601Z"
    }
   },
   "id": "a111f97918a643f5",
   "execution_count": 210
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Przygotowanie movieId dla datasetów"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0e12553f9e350b0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 198832 entries, 0 to 198831\n",
      "Data columns (total 29 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   userId                   198832 non-null  int64  \n",
      " 1   num_rating               198832 non-null  float64\n",
      " 2   avg_rating               198832 non-null  float64\n",
      " 3   weekend_watcher          198832 non-null  float64\n",
      " 4   genre_Action             198832 non-null  float64\n",
      " 5   genre_Adventure          198832 non-null  float64\n",
      " 6   genre_Animation          198832 non-null  float64\n",
      " 7   genre_Comedy             198832 non-null  float64\n",
      " 8   genre_Crime              198832 non-null  float64\n",
      " 9   genre_Documentary        198832 non-null  float64\n",
      " 10  genre_Drama              198832 non-null  float64\n",
      " 11  genre_Family             198832 non-null  float64\n",
      " 12  genre_Fantasy            198832 non-null  float64\n",
      " 13  genre_History            198832 non-null  float64\n",
      " 14  genre_Horror             198832 non-null  float64\n",
      " 15  genre_Music              198832 non-null  float64\n",
      " 16  genre_Mystery            198832 non-null  float64\n",
      " 17  genre_Romance            198832 non-null  float64\n",
      " 18  genre_Science Fiction    198832 non-null  float64\n",
      " 19  genre_TV Movie           198832 non-null  float64\n",
      " 20  genre_Thriller           198832 non-null  float64\n",
      " 21  genre_War                198832 non-null  float64\n",
      " 22  genre_Western            198832 non-null  float64\n",
      " 23  type_of_viewer_negative  198832 non-null  float64\n",
      " 24  type_of_viewer_neutral   198832 non-null  float64\n",
      " 25  type_of_viewer_positive  198832 non-null  float64\n",
      " 26  movies_seq               198832 non-null  object \n",
      " 27  ratings_seq              198832 non-null  object \n",
      " 28  ts_seq                   198832 non-null  object \n",
      "dtypes: float64(25), int64(1), object(3)\n",
      "memory usage: 44.0+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 198832 entries, 0 to 198831\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   userId  198832 non-null  int64 \n",
      " 1   pos     198832 non-null  object\n",
      " 2   neg     198832 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 4.6+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82918 entries, 0 to 82917\n",
      "Data columns (total 29 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   movieId              82918 non-null  int64  \n",
      " 1   runtime              82918 non-null  float64\n",
      " 2   if_blockbuster       82918 non-null  int64  \n",
      " 3   highly_watched       82918 non-null  int64  \n",
      " 4   highly_rated         82918 non-null  int64  \n",
      " 5   engagement_score     82918 non-null  float64\n",
      " 6   cast_importance      82918 non-null  float64\n",
      " 7   director_score       82918 non-null  float64\n",
      " 8   has_keywords         82918 non-null  int64  \n",
      " 9   has_cast             82918 non-null  int64  \n",
      " 10  has_director         82918 non-null  int64  \n",
      " 11  genre_ids            82918 non-null  object \n",
      " 12  decade_[1890, 1900)  82918 non-null  bool   \n",
      " 13  decade_[1900, 1910)  82918 non-null  bool   \n",
      " 14  decade_[1910, 1920)  82918 non-null  bool   \n",
      " 15  decade_[1920, 1930)  82918 non-null  bool   \n",
      " 16  decade_[1930, 1940)  82918 non-null  bool   \n",
      " 17  decade_[1940, 1950)  82918 non-null  bool   \n",
      " 18  decade_[1950, 1960)  82918 non-null  bool   \n",
      " 19  decade_[1960, 1970)  82918 non-null  bool   \n",
      " 20  decade_[1970, 1980)  82918 non-null  bool   \n",
      " 21  decade_[1980, 1990)  82918 non-null  bool   \n",
      " 22  decade_[1990, 2000)  82918 non-null  bool   \n",
      " 23  decade_[2000, 2010)  82918 non-null  bool   \n",
      " 24  decade_[2010, 2020)  82918 non-null  bool   \n",
      " 25  decade_[2020, 2030)  82918 non-null  bool   \n",
      " 26  text_embedded        82918 non-null  object \n",
      " 27  actor_ids            82918 non-null  object \n",
      " 28  director_ids         82918 non-null  object \n",
      "dtypes: bool(14), float64(4), int64(7), object(4)\n",
      "memory usage: 10.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_users.info())\n",
    "print(df_ratings.info())\n",
    "print(df_movies.info())\n",
    "\n",
    "empty_pos_ratings = df_ratings['pos'].apply(lambda x: len(x) == 0).sum()\n",
    "empty_neg_ratings = df_ratings['neg'].apply(lambda x: len(x) == 0).sum()\n",
    "\n",
    "if empty_pos_ratings != 0 or empty_neg_ratings != 0:\n",
    "    print(f'Empty ratings: pos: {empty_pos_ratings}, neg: {empty_neg_ratings}')\n",
    "    raise Exception(\"Users without a single pos/neg rating exist in the ratings_groupped_ids dataset\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T15:59:44.379376Z",
     "start_time": "2025-05-09T15:59:44.234557Z"
    }
   },
   "id": "3048e4d029e71c69",
   "execution_count": 211
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique movieIds: 82932\n",
      "min idx: 0\n",
      "max idx: 82931\n"
     ]
    }
   ],
   "source": [
    "unique_ids = set(\n",
    "        df_users['movies_seq'].explode().tolist()\n",
    "        + df_ratings['pos'].explode().tolist() \n",
    "        + df_ratings['neg'].explode().tolist()\n",
    "    )\n",
    "\n",
    "print('Unique movieIds:', len(unique_ids))\n",
    "unique_ids = sorted(unique_ids)\n",
    "\n",
    "movieId_to_idx = {id_: idx for idx, id_ in enumerate(unique_ids)}\n",
    "print('min idx:', min(movieId_to_idx.values()))\n",
    "print('max idx:', max(movieId_to_idx.values()))\n",
    "\n",
    "n_items = len(unique_ids)\n",
    "\n",
    "assert min(movieId_to_idx.values()) == 0\n",
    "assert max(movieId_to_idx.values()) == n_items - 1\n",
    "\n",
    "# unique_ids = sorted(df_movies['movieId'].unique())\n",
    "# movieId_to_idx = {id_: idx for idx, id_ in enumerate(unique_ids)}\n",
    "# n_items = len(movieId_to_idx)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T15:59:50.913753Z",
     "start_time": "2025-05-09T15:59:44.381383Z"
    }
   },
   "id": "26555242a1bf8026",
   "execution_count": 212
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Zmapuj movieId do indeksów\n",
    "df_users['movies_seq'] = df_users['movies_seq'].apply(lambda lst: [movieId_to_idx[m] for m in lst])\n",
    "df_ratings['pos'] = df_ratings['pos'].apply(lambda lst: [movieId_to_idx[m] for m in lst])\n",
    "df_ratings['neg'] = df_ratings['neg'].apply(lambda lst: [movieId_to_idx[m] for m in lst])\n",
    "\n",
    "# df_movies musi być ograniczone tylko do używanych filmów\n",
    "df_movies = df_movies[df_movies['movieId'].isin(movieId_to_idx)]\n",
    "df_movies['movie_idx'] = df_movies['movieId'].map(movieId_to_idx)\n",
    "\n",
    "# Final sanity check\n",
    "assert df_users['movies_seq'].explode().max() < n_items\n",
    "assert df_ratings['pos'].explode().max() < n_items\n",
    "assert df_ratings['neg'].explode().max() < n_items\n",
    "assert df_movies['movie_idx'].max() < n_items\n",
    "assert df_movies['movie_idx'].notna().all(), \"Some movieIds weren't mapped!\"\n",
    "\n",
    "\n",
    "# df_users['movies_seq'] = df_users['movies_seq'].apply(lambda lst: [movieId_to_idx[m] for m in lst if m in movieId_to_idx])\n",
    "# df_ratings['pos'] = df_ratings['pos'].apply(lambda lst: [movieId_to_idx[m] for m in lst if m in movieId_to_idx])\n",
    "# df_ratings['neg'] = df_ratings['neg'].apply(lambda lst: [movieId_to_idx[m] for m in lst if m in movieId_to_idx])\n",
    "# \n",
    "# assert df_users['movies_seq'].explode().max() < n_items"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:02.725647Z",
     "start_time": "2025-05-09T15:59:50.915841Z"
    }
   },
   "id": "47393b9900a89701",
   "execution_count": 213
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_movie_idx = 82931\n",
      "n_items = 82932\n"
     ]
    }
   ],
   "source": [
    "max_movie_idx = df_users['movies_seq'].explode().max()\n",
    "print(\"max_movie_idx =\", max_movie_idx)\n",
    "print(\"n_items =\", n_items)\n",
    "\n",
    "assert max_movie_idx < n_items, \"Indeks filmu przekracza rozmiar embeddingu\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.135247Z",
     "start_time": "2025-05-09T16:00:02.726667Z"
    }
   },
   "id": "2662480c39d6c125",
   "execution_count": 214
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zawiera niepoprawne wartości: False\n"
     ]
    }
   ],
   "source": [
    "def has_invalid_entries(seq_col):\n",
    "    return seq_col.explode().isin([-1, np.nan, None]).any()\n",
    "\n",
    "print(\"Zawiera niepoprawne wartości:\", has_invalid_entries(df_users['movies_seq']))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.523883Z",
     "start_time": "2025-05-09T16:00:03.136253Z"
    }
   },
   "id": "f0fa771088cf1bf1",
   "execution_count": 215
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82918 entries, 0 to 82917\n",
      "Data columns (total 30 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   movieId              82918 non-null  int64  \n",
      " 1   runtime              82918 non-null  float64\n",
      " 2   if_blockbuster       82918 non-null  int64  \n",
      " 3   highly_watched       82918 non-null  int64  \n",
      " 4   highly_rated         82918 non-null  int64  \n",
      " 5   engagement_score     82918 non-null  float64\n",
      " 6   cast_importance      82918 non-null  float64\n",
      " 7   director_score       82918 non-null  float64\n",
      " 8   has_keywords         82918 non-null  int64  \n",
      " 9   has_cast             82918 non-null  int64  \n",
      " 10  has_director         82918 non-null  int64  \n",
      " 11  genre_ids            82918 non-null  object \n",
      " 12  decade_[1890, 1900)  82918 non-null  bool   \n",
      " 13  decade_[1900, 1910)  82918 non-null  bool   \n",
      " 14  decade_[1910, 1920)  82918 non-null  bool   \n",
      " 15  decade_[1920, 1930)  82918 non-null  bool   \n",
      " 16  decade_[1930, 1940)  82918 non-null  bool   \n",
      " 17  decade_[1940, 1950)  82918 non-null  bool   \n",
      " 18  decade_[1950, 1960)  82918 non-null  bool   \n",
      " 19  decade_[1960, 1970)  82918 non-null  bool   \n",
      " 20  decade_[1970, 1980)  82918 non-null  bool   \n",
      " 21  decade_[1980, 1990)  82918 non-null  bool   \n",
      " 22  decade_[1990, 2000)  82918 non-null  bool   \n",
      " 23  decade_[2000, 2010)  82918 non-null  bool   \n",
      " 24  decade_[2010, 2020)  82918 non-null  bool   \n",
      " 25  decade_[2020, 2030)  82918 non-null  bool   \n",
      " 26  text_embedded        82918 non-null  object \n",
      " 27  actor_ids            82918 non-null  object \n",
      " 28  director_ids         82918 non-null  object \n",
      " 29  movie_idx            82918 non-null  int64  \n",
      "dtypes: bool(14), float64(4), int64(8), object(4)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_movies.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.551521Z",
     "start_time": "2025-05-09T16:00:03.525897Z"
    }
   },
   "id": "34768a6fd2aa2c3",
   "execution_count": 216
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#FOR QUICK TEST's\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "if DEBUG:\n",
    "    df_users = df_users.sample(n=256, random_state=42).copy()\n",
    "    df_ratings = df_ratings[df_ratings['userId'].isin(df_users['userId'])].copy()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.559433Z",
     "start_time": "2025-05-09T16:00:03.553617Z"
    }
   },
   "id": "bf7eb31cf0a19af9",
   "execution_count": 217
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Przygotowanie danych (Item Tower)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4986dac403eb091b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def prepare_feature_tensor(df_movies: pd.DataFrame):\n",
    "    import ast\n",
    "\n",
    "    for col in ['text_embedded', 'genre_ids', 'actor_ids', 'director_ids']:\n",
    "        df_movies[col] = df_movies[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Mapowania ID dla Embeedings\n",
    "    all_actor_ids = set(i for sub in df_movies['actor_ids'] for i in sub)\n",
    "    all_director_ids = set(i for sub in df_movies['director_ids'] for i in sub)\n",
    "    all_genre_ids = set(i for sub in df_movies['genre_ids'] for i in sub)\n",
    "\n",
    "    actor_id_map = {aid: idx for idx, aid in enumerate(sorted(all_actor_ids))}\n",
    "    director_id_map = {did: idx for idx, did in enumerate(sorted(all_director_ids))}\n",
    "    genre_id_map = {gid: idx for idx, gid in enumerate(sorted(all_genre_ids))}\n",
    "    \n",
    "    # Map the raw IDs to internal indices\n",
    "    df_movies['actor_ids'] = df_movies['actor_ids'].apply(lambda lst: [actor_id_map[i] for i in lst])\n",
    "    df_movies['director_ids'] = df_movies['director_ids'].apply(lambda lst: [director_id_map[i] for i in lst])\n",
    "    df_movies['genre_ids'] = df_movies['genre_ids'].apply(lambda lst: [genre_id_map[i] for i in lst])\n",
    "\n",
    "    # EmbeddingBag\n",
    "    def make_bag_inputs(id_lists):\n",
    "        flat = []\n",
    "        offsets = [0]\n",
    "        for lst in id_lists:\n",
    "            flat.extend(lst)\n",
    "            offsets.append(len(flat))\n",
    "        return torch.tensor(flat, dtype=torch.long), torch.tensor(offsets[:-1], dtype=torch.long)\n",
    "\n",
    "    actor_idx_bag, actor_offsets = make_bag_inputs(df_movies['actor_ids'])\n",
    "    director_idx_bag, director_offsets = make_bag_inputs(df_movies['director_ids'])\n",
    "    genre_idx_bag, genre_offsets = make_bag_inputs(df_movies['genre_ids'])\n",
    "\n",
    "    text_tensor = np.stack(df_movies['text_embedded'].apply(np.array).to_list())\n",
    "\n",
    "    numeric_cols = ['runtime', 'engagement_score', 'cast_importance', 'director_score']\n",
    "    binary_cols = ['if_blockbuster', 'highly_watched', 'highly_rated', 'has_keywords', 'has_cast', 'has_director']\n",
    "    decade_cols = [col for col in df_movies.columns if col.startswith(\"decade_\")]\n",
    "\n",
    "    num_bin_tensor = df_movies[numeric_cols + binary_cols + decade_cols].astype(np.float32).values\n",
    "    full_features = np.hstack([num_bin_tensor, text_tensor])\n",
    "    features_tensor = torch.tensor(full_features, dtype=torch.float32)\n",
    "\n",
    "    if torch.isnan(features_tensor).any():\n",
    "        print(\"NaN in feature tensor!\")\n",
    "        features_tensor = torch.nan_to_num(features_tensor)\n",
    "\n",
    "    # assert actor_idx_bag.max().item() < num_actors, \"Actor ID exceeds num_actors\"\n",
    "    \n",
    "    return (features_tensor,\n",
    "            actor_idx_bag, actor_offsets,\n",
    "            director_idx_bag, director_offsets,\n",
    "            genre_idx_bag, genre_offsets,\n",
    "            len(actor_id_map), len(director_id_map), len(genre_id_map))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.573090Z",
     "start_time": "2025-05-09T16:00:03.561440Z"
    }
   },
   "id": "af472cc4a35d5e8c",
   "execution_count": 218
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_embedding_bag_inputs(indices, bag_tensor, offset_tensor):\n",
    "    new_offsets = []\n",
    "    new_bag = []\n",
    "    offset = 0\n",
    "    for i in indices:\n",
    "        i = i.item()\n",
    "        start = offset_tensor[i].item()\n",
    "        end = offset_tensor[i + 1].item() if i + 1 < len(offset_tensor) else len(bag_tensor)\n",
    "        segment = bag_tensor[start:end]\n",
    "        new_bag.extend(segment.tolist())\n",
    "        new_offsets.append(offset)\n",
    "        offset += len(segment)\n",
    "    return torch.tensor(new_bag, dtype=torch.long), torch.tensor(new_offsets, dtype=torch.long)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.582646Z",
     "start_time": "2025-05-09T16:00:03.576098Z"
    }
   },
   "id": "7c7810a1c18d7a90",
   "execution_count": 219
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Przygotowanie danych (User Tower)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e7a700257576a49"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class UserDataset(Dataset):\n",
    "    def __init__(self, df_users):\n",
    "        self.data = df_users\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data.iloc[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.590532Z",
     "start_time": "2025-05-09T16:00:03.584657Z"
    }
   },
   "id": "b978b68de16365c7",
   "execution_count": 220
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n_items = len(unique_ids)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    df_users, movies, ratings, timestamps, pos, neg = [], [], [], [], [], []\n",
    "\n",
    "    for row in batch:\n",
    "        movies.append(torch.tensor(row['movies_seq'], dtype=torch.long))\n",
    "        ratings.append(torch.tensor(row['ratings_seq'], dtype=torch.float32))\n",
    "        timestamps.append(torch.tensor(row['ts_seq'], dtype=torch.float32))\n",
    "\n",
    "        userId = row['userId']\n",
    "\n",
    "        r = row[['num_rating', 'avg_rating', 'weekend_watcher', 'genre_Action', 'genre_Adventure', 'genre_Animation', 'genre_Comedy', 'genre_Crime', 'genre_Documentary', 'genre_Drama', 'genre_Family', 'genre_Fantasy', 'genre_History', 'genre_Horror', 'genre_Music', 'genre_Mystery', 'genre_Romance', 'genre_Science Fiction', 'genre_TV Movie', 'genre_Thriller', 'genre_War', 'genre_Western', 'type_of_viewer_negative', 'type_of_viewer_neutral', 'type_of_viewer_positive']]\n",
    "        r = r.astype('float32').values\n",
    "\n",
    "        df_users.append(torch.tensor(r, dtype=torch.float32))\n",
    "        \n",
    "        # Get a random movieId that was rated positively and one that was rated negatively. \n",
    "        # Used during training to calculate BPR loss. \n",
    "        posAndNegRow = df_ratings[df_ratings['userId'] == userId].iloc[0]\n",
    "        pos.append(torch.tensor(random.choice(posAndNegRow['pos']), dtype=torch.long))\n",
    "        neg.append(torch.tensor(random.choice(posAndNegRow['neg']), dtype=torch.long))\n",
    "\n",
    "    return {\n",
    "        \"input\": {\n",
    "            \"df_users\": torch.stack(df_users),\n",
    "            \"movies\": torch.stack(movies),\n",
    "            \"ratings\": torch.stack(ratings),\n",
    "            \"timestamps\": torch.stack(timestamps),\n",
    "        },\n",
    "        \"pos\": torch.as_tensor(pos, dtype=torch.long),\n",
    "        \"neg\": torch.as_tensor(neg, dtype=torch.long)\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.601776Z",
     "start_time": "2025-05-09T16:00:03.591582Z"
    }
   },
   "id": "4301ce2313a54a58",
   "execution_count": 221
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model (Item Tower)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc98e5ac0c98868c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ItemTower(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=64, num_actors=10000, num_directors=5000, num_genres=19):\n",
    "        super(ItemTower, self).__init__()\n",
    "        self.actor_embedding = nn.EmbeddingBag(num_actors, 32, mode='mean')\n",
    "        self.director_embedding = nn.EmbeddingBag(num_directors, 32, mode='mean')\n",
    "        self.genre_embedding = nn.EmbeddingBag(num_genres, 16, mode='mean')\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + 32 + 32 + 16, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, actor_bag, actor_offsets,\n",
    "                      director_bag, director_offsets,\n",
    "                      genre_bag, genre_offsets):\n",
    "        actor_emb = self.actor_embedding(actor_bag, actor_offsets)\n",
    "        director_emb = self.director_embedding(director_bag, director_offsets)\n",
    "        genre_emb = self.genre_embedding(genre_bag, genre_offsets)\n",
    "\n",
    "        x = torch.cat([x, actor_emb, director_emb, genre_emb], dim=1)\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.611953Z",
     "start_time": "2025-05-09T16:00:03.602781Z"
    }
   },
   "id": "b1924ffb59b1c8d3",
   "execution_count": 222
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model (User Tower)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bff0cda04322d8b6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class UserTower(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=64, n_items=1000):\n",
    "        super(UserTower, self).__init__()\n",
    "\n",
    "        # Item Embeddings for User History\n",
    "        self.item_emb = nn.Embedding(n_items, embedding_dim)\n",
    "        \n",
    "        # A layer to project rating and timestamp into a scalar weight\n",
    "        self.rating_proj = nn.Linear(2, 1)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim + embedding_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 384),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(384, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # Embed movieIds liked by user\n",
    "        m = self.item_emb(batch['movies'])\n",
    "\n",
    "        # Get weights from rating and timestamp\n",
    "        x = torch.stack([batch['ratings'], batch['timestamps']], dim=-1)\n",
    "        w = torch.sigmoid(self.rating_proj(x))\n",
    "\n",
    "        # weighted mean-pool\n",
    "        pooled = (m * w).sum(1) / (w.sum(1).clamp_min(1e-6))\n",
    "\n",
    "        input = torch.cat([batch['df_users'], pooled], dim=-1)\n",
    "        output = self.mlp(input)\n",
    "        u = F.normalize(output, dim=1)\n",
    "        return u"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.621752Z",
     "start_time": "2025-05-09T16:00:03.614063Z"
    }
   },
   "id": "f4a0454255a8c296",
   "execution_count": 223
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4847ef6afdfceeb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_two_tower_model_batched(\n",
    "    user_tower, item_tower,\n",
    "    df_users, df_ratings, df_movies,\n",
    "    movie_id_to_idx,\n",
    "    movie_features, actor_idx_bag, actor_offsets,\n",
    "    director_idx_bag, director_offsets,\n",
    "    genre_idx_bag, genre_offsets,\n",
    "    top_k=10, max_users=500, batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "\n",
    "    user_tower.eval()\n",
    "    item_tower.eval()\n",
    "\n",
    "    # PRECOMPUTED\n",
    "    with torch.no_grad():\n",
    "        item_embeddings = item_tower(\n",
    "            movie_features.to(device),\n",
    "            actor_idx_bag.to(device), actor_offsets.to(device),\n",
    "            director_idx_bag.to(device), director_offsets.to(device),\n",
    "            genre_idx_bag.to(device), genre_offsets.to(device)\n",
    "        )\n",
    "        item_embeddings = F.normalize(item_embeddings, dim=1)\n",
    "\n",
    "    print(\"Evaluating users in batches...\")\n",
    "    user_ids = df_users['userId'].unique()\n",
    "    if len(user_ids) > max_users:\n",
    "        user_ids = np.random.choice(user_ids, size=max_users, replace=False)\n",
    "\n",
    "    metrics = {'Precision@K': [], 'Recall@K': [], 'MRR': [], 'nDCG@K': []}\n",
    "\n",
    "    def precision_at_k(true_item, recommended, k): return int(true_item in recommended[:k]) / k\n",
    "    \n",
    "    def recall_at_k(true_item, recommended, k): return int(true_item in recommended[:k])\n",
    "    \n",
    "    def mrr(true_item, recommended): return 1 / (recommended.index(true_item) + 1) if true_item in recommended else 0\n",
    "    \n",
    "    def ndcg_at_k(true_item, recommended, k):\n",
    "        if true_item in recommended[:k]:\n",
    "            rank = recommended.index(true_item)\n",
    "            return 1 / np.log2(rank + 2)\n",
    "        return 0.0\n",
    "\n",
    "    for i in tqdm(range(0, len(user_ids), batch_size)):\n",
    "        batch_user_ids = user_ids[i:i+batch_size]\n",
    "        batch_inputs = {'movies': [], 'ratings': [], 'timestamps': [], 'df_users': []}\n",
    "        held_out_items = []\n",
    "        history_indices = []\n",
    "\n",
    "        for user_id in batch_user_ids:\n",
    "            row_u = df_users[df_users['userId'] == user_id].iloc[0]\n",
    "            row_r = df_ratings[df_ratings['userId'] == user_id].iloc[0]\n",
    "            pos_movies = row_r['pos']\n",
    "            if len(pos_movies) < 2:\n",
    "                continue\n",
    "\n",
    "            held_out = pos_movies[-1]\n",
    "            history = pos_movies[:-1]\n",
    "\n",
    "            indices = [movie_id_to_idx[mid] for mid in history if mid in movie_id_to_idx]\n",
    "            if not indices:\n",
    "                continue\n",
    "\n",
    "            batch_inputs['movies'].append(torch.tensor(indices, dtype=torch.long))\n",
    "            batch_inputs['ratings'].append(torch.tensor([4.0]*len(indices), dtype=torch.float32))\n",
    "            batch_inputs['timestamps'].append(torch.tensor([1.0]*len(indices), dtype=torch.float32))\n",
    "            batch_inputs['df_users'].append(torch.tensor(row_u.drop(['userId', 'movies_seq', 'ratings_seq', 'ts_seq']).values.astype(np.float32)))\n",
    "\n",
    "            held_out_items.append(held_out)\n",
    "            history_indices.append(indices)\n",
    "\n",
    "        if not held_out_items:\n",
    "            continue\n",
    "\n",
    "        max_len = max(len(seq) for seq in batch_inputs['movies'])\n",
    "        for key in ['movies', 'ratings', 'timestamps']:\n",
    "            batch_inputs[key] = torch.stack([\n",
    "                F.pad(seq, (0, max_len - len(seq)), value=0) for seq in batch_inputs[key]\n",
    "            ])\n",
    "\n",
    "        batch_inputs['df_users'] = torch.stack(batch_inputs['df_users']).to(device)\n",
    "        batch_inputs['movies'] = batch_inputs['movies'].to(device)\n",
    "        batch_inputs['ratings'] = batch_inputs['ratings'].to(device)\n",
    "        batch_inputs['timestamps'] = batch_inputs['timestamps'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            user_vecs = user_tower(batch_inputs)\n",
    "            scores = torch.matmul(user_vecs, item_embeddings.T)\n",
    "\n",
    "            for j, user_score in enumerate(scores):\n",
    "                # Mask history\n",
    "                user_score[history_indices[j]] = -1e9\n",
    "                top_k_items = torch.topk(user_score, k=top_k).indices.tolist()\n",
    "                top_k_movie_ids = [df_movies.iloc[x]['movieId'] for x in top_k_items]\n",
    "\n",
    "                true_item = held_out_items[j]\n",
    "                metrics['Precision@K'].append(precision_at_k(true_item, top_k_movie_ids, top_k))\n",
    "                metrics['Recall@K'].append(recall_at_k(true_item, top_k_movie_ids, top_k))\n",
    "                metrics['MRR'].append(mrr(true_item, top_k_movie_ids))\n",
    "                metrics['nDCG@K'].append(ndcg_at_k(true_item, top_k_movie_ids, top_k))\n",
    "\n",
    "    return {k: np.mean(v) if v else 0.0 for k, v in metrics.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.639024Z",
     "start_time": "2025-05-09T16:00:03.622782Z"
    }
   },
   "id": "e513cef153f61a1e",
   "execution_count": 224
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRAINING"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f113206c7c12e94a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "\n",
    "print('Device:', device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.645846Z",
     "start_time": "2025-05-09T16:00:03.640031Z"
    }
   },
   "id": "93b3ac51d07a6822",
   "execution_count": 225
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    if isinstance(data, dict):\n",
    "        return {k: to_device(v, device) for k, v in data.items()}\n",
    "    elif torch.is_tensor(data):\n",
    "        return data.to(device)\n",
    "    else:\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:03.652814Z",
     "start_time": "2025-05-09T16:00:03.646854Z"
    }
   },
   "id": "63b81accca4047d6",
   "execution_count": 226
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4096 #DEBUG: 32\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df_users, test_size=0.2)\n",
    "\n",
    "movie_features, actor_idx_bag, actor_offsets, director_idx_bag, director_offsets, genre_idx_bag, genre_offsets, num_actors, num_directors, num_genres = prepare_feature_tensor(df_movies)\n",
    "\n",
    "trainDataset = UserDataset(train_df)\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "testDataset = UserDataset(test_df)\n",
    "testDataLoader = DataLoader(testDataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:05.494704Z",
     "start_time": "2025-05-09T16:00:03.653853Z"
    }
   },
   "id": "61bc74d89fe41682",
   "execution_count": 227
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "EMB_DIM = 128\n",
    "\n",
    "user_tower = UserTower(input_dim=25, n_items=n_items, embedding_dim=EMB_DIM).to(device)\n",
    "item_tower = ItemTower(\n",
    "    input_dim=movie_features.shape[1],\n",
    "    embedding_dim=EMB_DIM,\n",
    "    num_actors=num_actors,\n",
    "    num_directors=num_directors,\n",
    "    num_genres=num_genres\n",
    ").to(device)\n",
    "\n",
    "\n",
    "params = list(user_tower.parameters()) + list(item_tower.parameters())\n",
    "optimizer = optim.Adam(params, lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:05.659563Z",
     "start_time": "2025-05-09T16:00:05.495716Z"
    }
   },
   "id": "d7cfbda3b4199933",
   "execution_count": 228
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_one_epoch_two_tower(user_tower, item_tower, data_loader, optimizer, device, movie_features,\n",
    "                              actor_idx_bag, actor_offsets,\n",
    "                              director_idx_bag, director_offsets,\n",
    "                              genre_idx_bag, genre_offsets):\n",
    "    \n",
    "    user_tower.train()\n",
    "    item_tower.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    \n",
    "    movie_features = movie_features.to(device)\n",
    "    actor_idx_bag = actor_idx_bag.to(device)\n",
    "    actor_offsets = actor_offsets.to(device)\n",
    "    director_idx_bag = director_idx_bag.to(device)\n",
    "    director_offsets = director_offsets.to(device)\n",
    "    genre_idx_bag = genre_idx_bag.to(device)\n",
    "    genre_offsets = genre_offsets.to(device)\n",
    "\n",
    "    for batch in data_loader:\n",
    "        batch = to_device(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        user_vec = user_tower(batch['input'])\n",
    "\n",
    "        actor_pos_bag, actor_pos_offsets = get_embedding_bag_inputs(batch['pos'], actor_idx_bag, actor_offsets)\n",
    "        director_pos_bag, director_pos_offsets = get_embedding_bag_inputs(batch['pos'], director_idx_bag, director_offsets)\n",
    "        genre_pos_bag, genre_pos_offsets = get_embedding_bag_inputs(batch['pos'], genre_idx_bag, genre_offsets)\n",
    "\n",
    "        actor_neg_bag, actor_neg_offsets = get_embedding_bag_inputs(batch['neg'], actor_idx_bag, actor_offsets)\n",
    "        director_neg_bag, director_neg_offsets = get_embedding_bag_inputs(batch['neg'], director_idx_bag, director_offsets)\n",
    "        genre_neg_bag, genre_neg_offsets = get_embedding_bag_inputs(batch['neg'], genre_idx_bag, genre_offsets)\n",
    "        \n",
    "        #FOR DEBUGGING\n",
    "        # print(\"max actor id in batch:\", actor_pos_bag.max().item(), \"num_actors:\", item_tower.actor_embedding.num_embeddings)\n",
    "\n",
    "        pos_vec = item_tower(movie_features[batch['pos']].to(device), actor_pos_bag.to(device), actor_pos_offsets.to(device),\n",
    "                             director_pos_bag.to(device), director_pos_offsets.to(device),\n",
    "                             genre_pos_bag.to(device), genre_pos_offsets.to(device))\n",
    "        \n",
    "        neg_vec = item_tower(movie_features[batch['neg']].to(device), actor_neg_bag.to(device), actor_neg_offsets.to(device),\n",
    "                             director_neg_bag.to(device), director_neg_offsets.to(device),\n",
    "                             genre_neg_bag.to(device), genre_neg_offsets.to(device))\n",
    "\n",
    "\n",
    "        pos_score = (user_vec * pos_vec).sum(dim=-1)\n",
    "        neg_score = (user_vec * neg_vec).sum(dim=-1)\n",
    "\n",
    "        loss = -F.logsigmoid(pos_score - neg_score).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += 1\n",
    "\n",
    "    return running_loss / total"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-09T16:00:05.672817Z",
     "start_time": "2025-05-09T16:00:05.661570Z"
    }
   },
   "id": "5fa4ee0b0acef757",
   "execution_count": 229
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Two-Tower:   2%|▏         | 1/50 [06:50<5:35:00, 410.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] | Loss: 0.5869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Two-Tower:   4%|▍         | 2/50 [13:35<5:26:03, 407.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] | Loss: 0.2913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Two-Tower:   6%|▌         | 3/50 [20:14<5:16:05, 403.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] | Loss: 0.1359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Two-Tower:   8%|▊         | 4/50 [26:52<5:07:45, 401.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] | Loss: 0.0778\n",
      "[Epoch 5] | Loss: 0.0453\n",
      "[Epoch 5] Pointwise Eval:\n",
      "  ROC AUC:       0.6011\n",
      "  Pairwise Acc:  0.6015\n",
      "Evaluating users in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]\u001B[A\n",
      "  3%|▎         | 1/32 [00:00<00:07,  3.94it/s]\u001B[A\n",
      "  6%|▋         | 2/32 [00:00<00:07,  3.82it/s]\u001B[A\n",
      "  9%|▉         | 3/32 [00:00<00:08,  3.41it/s]\u001B[A\n",
      " 12%|█▎        | 4/32 [00:01<00:08,  3.47it/s]\u001B[A\n",
      " 16%|█▌        | 5/32 [00:01<00:07,  3.84it/s]\u001B[A\n",
      " 19%|█▉        | 6/32 [00:01<00:06,  4.12it/s]\u001B[A\n",
      " 22%|██▏       | 7/32 [00:01<00:05,  4.18it/s]\u001B[A\n",
      " 25%|██▌       | 8/32 [00:02<00:05,  4.05it/s]\u001B[A\n",
      " 28%|██▊       | 9/32 [00:02<00:05,  3.90it/s]\u001B[A\n",
      " 31%|███▏      | 10/32 [00:02<00:05,  4.03it/s]\u001B[A\n",
      " 34%|███▍      | 11/32 [00:02<00:04,  4.54it/s]\u001B[A\n",
      " 38%|███▊      | 12/32 [00:02<00:03,  5.27it/s]\u001B[A\n",
      " 41%|████      | 13/32 [00:02<00:03,  5.98it/s]\u001B[A\n",
      " 44%|████▍     | 14/32 [00:03<00:03,  5.84it/s]\u001B[A\n",
      " 47%|████▋     | 15/32 [00:03<00:02,  5.69it/s]\u001B[A\n",
      " 50%|█████     | 16/32 [00:03<00:02,  5.85it/s]\u001B[A\n",
      " 53%|█████▎    | 17/32 [00:03<00:02,  6.34it/s]\u001B[A\n",
      " 56%|█████▋    | 18/32 [00:03<00:02,  6.76it/s]\u001B[A\n",
      " 59%|█████▉    | 19/32 [00:03<00:01,  7.07it/s]\u001B[A\n",
      " 62%|██████▎   | 20/32 [00:03<00:01,  7.31it/s]\u001B[A\n",
      " 66%|██████▌   | 21/32 [00:04<00:01,  6.87it/s]\u001B[A\n",
      " 69%|██████▉   | 22/32 [00:04<00:01,  6.92it/s]\u001B[A\n",
      " 72%|███████▏  | 23/32 [00:04<00:01,  6.91it/s]\u001B[A\n",
      " 75%|███████▌  | 24/32 [00:04<00:01,  6.85it/s]\u001B[A\n",
      " 78%|███████▊  | 25/32 [00:04<00:00,  7.13it/s]\u001B[A\n",
      " 81%|████████▏ | 26/32 [00:04<00:00,  6.93it/s]\u001B[A\n",
      " 84%|████████▍ | 27/32 [00:05<00:00,  6.61it/s]\u001B[A\n",
      " 88%|████████▊ | 28/32 [00:05<00:00,  5.87it/s]\u001B[A\n",
      " 91%|█████████ | 29/32 [00:05<00:00,  5.42it/s]\u001B[A\n",
      " 94%|█████████▍| 30/32 [00:05<00:00,  5.10it/s]\u001B[A\n",
      "100%|██████████| 32/32 [00:05<00:00,  5.36it/s]\u001B[A\n",
      "Training Two-Tower:  10%|█         | 5/50 [35:10<5:27:01, 436.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Retrieval Eval:\n",
      "  Precision@K:   0.0000\n",
      "  Recall@K:      0.0000\n",
      "  MRR:           0.0000\n",
      "  nDCG@K:        0.0000\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "EPOCHS = 50\n",
    "EVAL_EVERY = 5\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), desc=\"Training Two-Tower\"):\n",
    "    avg_loss = train_one_epoch_two_tower(\n",
    "        user_tower=user_tower,\n",
    "        item_tower=item_tower,\n",
    "        data_loader=trainDataLoader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        movie_features = movie_features,\n",
    "        actor_idx_bag=actor_idx_bag,\n",
    "        actor_offsets=actor_offsets,\n",
    "        director_idx_bag=director_idx_bag,\n",
    "        director_offsets=director_offsets,\n",
    "        genre_idx_bag=genre_idx_bag,\n",
    "        genre_offsets=genre_offsets\n",
    "    )\n",
    "    \n",
    "    print(f\"[Epoch {epoch + 1}] | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    if epoch % EVAL_EVERY == (EVAL_EVERY - 1):\n",
    "        user_tower.eval()\n",
    "        item_tower.eval()\n",
    "\n",
    "        aucs, pair_accs = [], []\n",
    "        \n",
    "        movie_features = movie_features.to(device)\n",
    "        actor_idx_bag = actor_idx_bag.to(device)\n",
    "        actor_offsets = actor_offsets.to(device)\n",
    "        director_idx_bag = director_idx_bag.to(device)\n",
    "        director_offsets = director_offsets.to(device)\n",
    "        genre_idx_bag = genre_idx_bag.to(device)\n",
    "        genre_offsets = genre_offsets.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            item_emb = item_tower(\n",
    "                movie_features.to(device),\n",
    "                actor_idx_bag.to(device),\n",
    "                actor_offsets.to(device),\n",
    "                director_idx_bag.to(device),\n",
    "                director_offsets.to(device),\n",
    "                genre_idx_bag.to(device),\n",
    "                genre_offsets.to(device)\n",
    "            ).cpu().detach().numpy()\n",
    "\n",
    "            for batch in testDataLoader:\n",
    "                batch = to_device(batch, device)\n",
    "\n",
    "                u = user_tower(batch['input'])\n",
    "\n",
    "                actor_pos_bag, actor_pos_offsets = get_embedding_bag_inputs(batch['pos'], actor_idx_bag, actor_offsets)\n",
    "                director_pos_bag, director_pos_offsets = get_embedding_bag_inputs(batch['pos'], director_idx_bag,director_offsets)\n",
    "                genre_pos_bag, genre_pos_offsets = get_embedding_bag_inputs(batch['pos'], genre_idx_bag,genre_offsets)\n",
    "\n",
    "                actor_neg_bag, actor_neg_offsets = get_embedding_bag_inputs(batch['neg'], actor_idx_bag,actor_offsets)\n",
    "                director_neg_bag, director_neg_offsets = get_embedding_bag_inputs(batch['neg'], director_idx_bag, director_offsets)\n",
    "                genre_neg_bag, genre_neg_offsets = get_embedding_bag_inputs(batch['neg'], genre_idx_bag, genre_offsets)\n",
    "\n",
    "                pos_vec = item_tower(movie_features[batch['pos']].to(device), actor_pos_bag.to(device), actor_pos_offsets.to(device),\n",
    "                                     director_pos_bag.to(device), director_pos_offsets.to(device),\n",
    "                                     genre_pos_bag.to(device), genre_pos_offsets.to(device))\n",
    "                \n",
    "                neg_vec = item_tower(movie_features[batch['neg']].to(device), actor_neg_bag.to(device), actor_neg_offsets.to(device),\n",
    "                                     director_neg_bag.to(device), director_neg_offsets.to(device),\n",
    "                                     genre_neg_bag.to(device), genre_neg_offsets.to(device))\n",
    "\n",
    "                pos_score = (u * pos_vec).sum(dim=-1)\n",
    "                neg_score = (u * neg_vec).sum(dim=-1)\n",
    "\n",
    "                labels = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "                scores = torch.cat([pos_score, neg_score])\n",
    "\n",
    "                aucs.append(roc_auc_score(labels.cpu(), scores.cpu()))\n",
    "                pair_accs.append((pos_score > neg_score).float().mean().item())\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Pointwise Eval:\")\n",
    "        print(f\"  ROC AUC:       {np.mean(aucs):.4f}\")\n",
    "        print(f\"  Pairwise Acc:  {np.mean(pair_accs):.4f}\")\n",
    "\n",
    "        rank_metrics = evaluate_two_tower_model_batched(\n",
    "            user_tower=user_tower,\n",
    "            item_tower=item_tower,\n",
    "            df_users=test_df,           # ← has user profiles & sequences\n",
    "            df_ratings=df_ratings,      # ← has pos/neg lists\n",
    "            df_movies=df_movies,        # ← all movie features\n",
    "            movie_id_to_idx = df_movies.set_index('movieId')['movie_idx'].to_dict(),\n",
    "            top_k=10,\n",
    "            max_users=1000, #DEBUG: 32\n",
    "            batch_size=32, #DEBUG: 8\n",
    "            device=device,\n",
    "            movie_features=movie_features,\n",
    "            actor_idx_bag=actor_idx_bag,\n",
    "            actor_offsets=actor_offsets,\n",
    "            director_idx_bag=director_idx_bag,\n",
    "            director_offsets=director_offsets,\n",
    "            genre_idx_bag=genre_idx_bag,\n",
    "            genre_offsets=genre_offsets\n",
    "        )\n",
    "\n",
    "        print(f\"[Epoch {epoch + 1}] Retrieval Eval:\")\n",
    "        print(f\"  Precision@K:   {rank_metrics['Precision@K']:.4f}\")\n",
    "        print(f\"  Recall@K:      {rank_metrics['Recall@K']:.4f}\")\n",
    "        print(f\"  MRR:           {rank_metrics['MRR']:.4f}\")\n",
    "        print(f\"  nDCG@K:        {rank_metrics['nDCG@K']:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-05-09T16:00:05.674826Z"
    }
   },
   "id": "306e9e666441012b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(user_tower.state_dict(), f'user_tower_{timestamp}.pt')\n",
    "torch.save(item_tower.state_dict(), f'item_tower_{timestamp}.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "edb56e0f9c6c6e3b",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
